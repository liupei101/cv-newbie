# Deep Residual Learning for Image Recognition

## 介绍

深度的卷积神经网络已经在图像分类领域取得了多项突破。深度网络以一种端到端、多层的方式自然地将低、中、高级别的特征和分类器聚合在一起，并且特征的“等级”可以随着网络的加深而进一步丰富。许多证据或实验结果显示网络的深度对于深度神经网络模型是一个非常重要的因素。

随之而来的问题是：增加更多层，是否就直接意味着网络会学习地越好？回答这个问题的障碍在于我们所诟病的梯度消失、爆炸的问题，这从一开始就阻碍了网络的收敛。然而，这个问题被正态的初始化和BN中间层所解决。

当更加深的网络开始收敛，一个随之引发的问题开始出现：随着网络的加深，准确率开始饱和，然后开始下降。不可预料的是，这样的问题不是由过拟合导致的，并且增加更多的层到网络中会导致更高的训练误差。

训练集上出现这样的问题说明了不是所有系统都是那么容易去优化。让我们考虑一个较浅的架构及其更深层次的对象，为其添加更多的层。存在一种构建更深网络的解决方法：添加的层是恒等映射，其他层是从学习到的较浅模型的拷贝。 这种构造解决方案的存在表明，较深的模型不应该产生比其对应的较浅模型更高的训练误差。但是实验表明，我们目前现有的解决方案无法找到与构建的解决方案相比相对不错或更好的解决方案（或在合理的时间内无法实现）

本文，为了解决增加深度随之引发的问题，我们提出了deep residual learning framework。不在寄希望于每个多层网络组合去直接学习预期的映射，我们让这些网络层去学习残差的映射。更加正式地，将预期要学习的映射记为H(x)，我们让增加的非线性层去学习映射f(x) = h(x) - x。所以，原始预期要学习的映射可以表示为f(x) + x。某种程度上，如果一个等价的映射是最优的，那么把残差缩小为0比增加同样的多层线性映射层要更加容易。

上述的f(x) + x 的表达式可以由前向神经网络加上“Shortcut connections”实现。“Shortcut connections” 指忽略掉1层或多层中间层。这样的修改不会增加额外的网络参数，也不会增加计算复杂性。

我们提供了ImageNet上完整的实验结果去展示上面提到的问题，同时评估我们提出的方法。我们展示的内容包括
1. 我们的深度残差网络容易优化，而未加残差连接的网络容易随着深度的增加而出现较高的训练误差。
2. 我们的深度残差网络可以更加容易地从增加网络深度中获益，从而得到比原来网络明显更好的结果。

我们使用152层（已知最深的网络）的ResNet做ImageNet分类任务，最终以3.57% 的top5-error 赢得了ILSVRC-2015分类任务的冠军。同时在ImageNet detection 和 COCO detection 等比赛中均获得了第一名。这强有力地表明了残差学习策略是通用的，同时我们希望这项技术可以在其他的视觉或非视觉任务中得到应用。

![Residual learning: a building block (Shortcut connections)](tools/resnet-1.png)

## 深度残差学习

正如介绍中所指出的，我们需要学习的映射为 H(x)，如果使用多层的非线性层来学习残差，那么这个多层的非线性层映射就可以表示为 f(x) = H(x) - x。经过一些转化（由那个退化问题相关的反直觉现象得到出发），我们需要学习的映射就可以表示为 f(x) + x。这个Shortcut Connections的形式不仅在实际中非常引人注目，而且在我们比较带残差、不带残差网络的时候非常重要。加入这个Shortcut Connections并不影响模型参数，深度，宽度，以及计算消耗（除了多维数组逐个元素相加带来的计算消耗以外）。

当f(x)和x的维度不匹配时，我们可以改写上式为：y = f(x) + Wx。这里的W权值矩阵只在维度不匹配的时候使用。残差函数f的形式可以非常多样，一般为2层或3层的中间层，更多层也是可以的。但是只有一层的中间层时，这个Shortcut Connections就和线性层等价了（y = Wx + x），这就没有任何优势所在了。

## 实验

实验对比的带残差和不带残差的网络结构如下：

![Example network architectures for ImageNet](tools/resnet-2.png)

在残差网络中，当维度不匹配时，我们使用了两种方法：
1. 填充额外的0以增加维度；
2. 对x使用1 * 1的卷积来匹配维度。

对这两个方法，当连接跳过了2层时，他们相应的进行stride of 2。（对应图中的/2，作者在解释不带残差网络的模型时：We perform downsampling directly by
convolutional layers that have a stride of 2）

**实现细节**：
- 尺寸增强。图片根据短边重新调整了大小，短边的长度在[256, 480]内。
- 224 * 224 的裁剪随机地从图像中采样，以及采样后的水平翻转图片。同时，图片每个像素点都使用了均值减去。
- 标准的颜色增强方法也被使用。
- 我们在每个卷积层之后且激活层之前都采用了BN层。
- 权值的初始化参见文章[13]，每个mini-batch的大小为256，学习率初始值为0.1，每次error上升，学习率除10。
- 未使用dropout，优化算法中，权值衰减率为0.0001，动量为0.9。

**ImageNet上的实验结果**：

| Model | plain | ResNet |
|:-:|:-:|:-:|
| 18 layers | 27.94 | 27.88 |
| 34 layers | 28.54 | **25.03** |

上表给出了在ImageNet验证集上的Top-1 error（%，10-crop）。这里的ResNet没有任何的额外参数。下图展示了训练过程。

![Training on ImageNet](tools/resnet-3.png)

文章后文给出了以下讨论：
- 使用 补0 和 投影 两种不同的Shortcut Connections的模型比较
- ResNet-50，ResNet-101和ResNet-152在ImageNet数据集上的结果
- 与SOTA方法的比较：3.57% top-5 error赢得了ILSVRC-2015的冠军
- CIFAR-10数据集上的实验结果及分析

## 拓展

文章*K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In ECCV, 2016* 在ResNet的基础上，修改了Identity mappings的结构，达到了更好的实验效果，相关资料如下：

![ResNets Identity mappings variants](tools/resnet-4.png)

对其实现细节及解释感兴趣的朋友可以阅读原文。
